{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T02:25:21.180545Z",
     "start_time": "2024-09-12T02:25:03.615727Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mRunning cells with 'env (Python 3.9.13)' requires the ipykernel package.\n",
      "\u001B[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001B[1;31mCommand: '\"c:/Users/ADMIN/PycharmProjects/AI clone/env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# standardization\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "audio, sr = librosa.load(\"Data/output.wav\", sr=16000)\n",
    "\n",
    "normalized_audio = librosa.util.normalize(audio)\n",
    "\n",
    "sf.write(\"Standardization_data/normalized_voice.wav\", normalized_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edab6caf8cc7f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T07:15:16.193959Z",
     "start_time": "2024-09-12T07:15:15.993863Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mRunning cells with 'env (Python 3.9.13)' requires the ipykernel package.\n",
      "\u001B[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001B[1;31mCommand: '\"c:/Users/ADMIN/PycharmProjects/AI clone/env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "lstm_cell = tf.keras.layers.LSTMCell(units=128)"
   ]
  },
  {
   "cell_type": "code",
   "id": "420b5eaa8aa63ade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:02:26.790798Z",
     "start_time": "2024-09-25T07:02:26.695327Z"
    }
   },
   "source": [
    "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
    "config = GlowTTSConfig()\n",
    "config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlowTTSConfig(model='glow_tts', run_name='coqui_tts', run_description='', epochs=10000, batch_size=None, eval_batch_size=None, mixed_precision=False, scheduler_after_epoch=False, run_eval=True, test_delay_epochs=0, print_eval=False, dashboard_logger='tensorboard', print_step=25, plot_step=100, model_param_stats=False, project_name=None, log_model_step=None, wandb_entity=None, save_step=10000, checkpoint=True, keep_all_best=False, keep_after=10000, num_loader_workers=0, num_eval_loader_workers=0, use_noise_augment=False, output_path=None, distributed_backend='nccl', distributed_url='tcp://localhost:54321', audio=BaseAudioConfig(fft_size=1024, win_length=1024, hop_length=256, frame_shift_ms=None, frame_length_ms=None, stft_pad_mode='reflect', sample_rate=22050, resample=False, preemphasis=0.0, ref_level_db=20, do_sound_norm=False, log_func='np.log10', do_trim_silence=True, trim_db=45, power=1.5, griffin_lim_iters=60, num_mels=80, mel_fmin=0.0, mel_fmax=None, spec_gain=20, do_amp_to_db_linear=True, do_amp_to_db_mel=True, signal_norm=True, min_level_db=-100, symmetric_norm=True, max_norm=4.0, clip_norm=True, stats_path=None), use_phonemes=False, use_espeak_phonemes=True, phoneme_language=None, compute_input_seq_cache=False, text_cleaner=None, enable_eos_bos_chars=False, test_sentences_file='', phoneme_cache_path=None, characters=None, batch_group_size=0, loss_masking=None, sort_by_audio_len=False, min_seq_len=3, max_seq_len=500, compute_f0=False, compute_linear_spec=False, add_blank=False, datasets=[BaseDatasetConfig(name='', path='', meta_file_train='', ununsed_speakers=None, meta_file_val='', meta_file_attn_mask='')], optimizer='RAdam', optimizer_params={'betas': [0.9, 0.998], 'weight_decay': 1e-06}, lr_scheduler='NoamLR', lr_scheduler_params={'warmup_steps': 4000}, test_sentences=[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", 'Be a voice, not an echo.', \"I'm sorry Dave. I'm afraid I can't do that.\", \"This cake is great. It's so delicious and moist.\", 'Prior to November 22, 1963.'], num_chars=None, encoder_type='rel_pos_transformer', encoder_params={'kernel_size': 3, 'dropout_p': 0.1, 'num_layers': 6, 'num_heads': 2, 'hidden_channels_ffn': 768, 'input_length': None}, use_encoder_prenet=True, hidden_channels_enc=192, hidden_channels_dec=192, hidden_channels_dp=256, dropout_p_dp=0.1, dropout_p_dec=0.05, mean_only=True, out_channels=80, num_flow_blocks_dec=12, inference_noise_scale=0.0, kernel_size_dec=5, dilation_rate=1, num_block_layers=4, num_speakers=0, c_in_channels=0, num_splits=4, num_squeeze=2, sigmoid_scale=False, d_vector_dim=0, data_dep_init_steps=10, style_wav_for_test=None, length_scale=1.0, use_speaker_embedding=False, use_d_vector_file=False, d_vector_file=False, grad_clip=5.0, lr=0.001, r=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3260545186e0180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T08:28:50.788285Z",
     "start_time": "2024-09-20T08:28:50.778812Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mRunning cells with 'env (Python 3.9.13)' requires the ipykernel package.\n",
      "\u001B[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001B[1;31mCommand: '\"c:/Users/ADMIN/PycharmProjects/AI clone/env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Hàm để tải cấu hình từ file JSON\n",
    "def load_json_config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee079ece9087dc0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:28:23.456783Z",
     "start_time": "2024-10-04T02:28:17.091738Z"
    }
   },
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    random_tensor_cuda = torch.rand((3, 3)).cuda()\n",
    "    print(random_tensor_cuda)\n",
    "else:\n",
    "    print(\"no cuda.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4591, 0.2735, 0.2582],\n",
      "        [0.5430, 0.2876, 0.5771],\n",
      "        [0.8827, 0.3153, 0.0828]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:28:26.277279Z",
     "start_time": "2024-10-04T02:28:26.270334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ],
   "id": "3be9bf9a754d785e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a777394209c83177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:28:27.342402Z",
     "start_time": "2024-10-04T02:28:27.336100Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(torch.cuda.current_device()) / 1024**2:.2f} MB\")\n",
    "    print(f\"Memory Cached: {torch.cuda.memory_reserved(torch.cuda.current_device()) / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Number of GPUs: 1\n",
      "Current GPU: 0\n",
      "Device Name: NVIDIA GeForce RTX 2050\n",
      "Memory Allocated: 0.00 MB\n",
      "Memory Cached: 2.00 MB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c0129b026b13e267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T05:00:24.429077Z",
     "start_time": "2024-09-25T05:00:21.124373Z"
    }
   },
   "source": [
    "from TTS.utils.synthesizer import Synthesizer\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "# Tạo đối tượng quản lý mô hình\n",
    "manager = ModelManager()\n",
    "\n",
    "# Tải mô hình đã được huấn luyện cho Tacotron 2 và HiFi-GAN\n",
    "tts_path = manager.download_model('tts_models/en/ljspeech/tacotron2-DDC')\n",
    "# tts_path = manager.download_model('tts_models/vi/vietTTS/vietTTS')\n",
    "vocoder_path = manager.download_model('vocoder_models/en/ljspeech/hifigan_v2')\n",
    "# vocoder_path=manager.download_model('vocoder_models/vi/vietTTS/hifigan_v2')\n",
    "\n",
    "# Khởi tạo bộ tổng hợp âm thanh (synthesizer)\n",
    "synthesizer = Synthesizer(\n",
    "    tts_checkpoint=tts_path[0],  # Đường dẫn tới tệp mô hình TTS .pth\n",
    "    tts_config_path=tts_path[1],  # Đường dẫn tới tệp cấu hình TTS .json\n",
    "    vocoder_checkpoint=vocoder_path[0],\n",
    "    vocoder_config = vocoder_path[1] # Đường dẫn tới tệp vocoder .pth\n",
    ")\n",
    "\n",
    "# Tạo giọng nói từ văn bản và lưu trực tiếp ra file\n",
    "# text = \"This is a voice cloning test.\"\n",
    "# output_path = \"output.wav\"\n",
    "# wav = synthesizer.tts(\"xin chào tui là bạn\") \n",
    "wav = synthesizer.tts(\"hello how are you and how did you do that thing\")\n",
    "\n",
    "# Sử dụng phương thức tts_to_file để tạo và lưu file âm thanh\n",
    "synthesizer.save_wav(wav, \"output.wav\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      " > Text splitted to sentences.\n",
      "['hello how are you and how did you do that thing']\n",
      " > Processing time: 2.2825026512145996\n",
      " > Real-time factor: 0.7588839484210181\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:22:12.875249Z",
     "start_time": "2024-09-25T07:22:12.856796Z"
    }
   },
   "cell_type": "code",
   "source": "manager.list_models()",
   "id": "5a5aa833e2aad6ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name format: type/language/dataset/model\n",
      " 1: tts_models/en/ek1/tacotron2\n",
      " 2: tts_models/en/ljspeech/tacotron2-DDC [already downloaded]\n",
      " 3: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
      " 4: tts_models/en/ljspeech/glow-tts\n",
      " 5: tts_models/en/ljspeech/speedy-speech\n",
      " 6: tts_models/en/ljspeech/tacotron2-DCA\n",
      " 7: tts_models/en/ljspeech/vits\n",
      " 8: tts_models/en/ljspeech/fast_pitch\n",
      " 9: tts_models/en/vctk/sc-glow-tts\n",
      " 10: tts_models/en/vctk/vits\n",
      " 11: tts_models/en/vctk/fast_pitch\n",
      " 12: tts_models/en/sam/tacotron-DDC\n",
      " 13: tts_models/es/mai/tacotron2-DDC\n",
      " 14: tts_models/fr/mai/tacotron2-DDC\n",
      " 15: tts_models/uk/mai/glow-tts\n",
      " 16: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
      " 17: tts_models/nl/mai/tacotron2-DDC\n",
      " 18: tts_models/de/thorsten/tacotron2-DCA\n",
      " 19: tts_models/ja/kokoro/tacotron2-DDC\n",
      " 20: vocoder_models/universal/libri-tts/wavegrad\n",
      " 21: vocoder_models/universal/libri-tts/fullband-melgan\n",
      " 22: vocoder_models/en/ek1/wavegrad\n",
      " 23: vocoder_models/en/ljspeech/multiband-melgan\n",
      " 24: vocoder_models/en/ljspeech/hifigan_v2 [already downloaded]\n",
      " 25: vocoder_models/en/ljspeech/univnet\n",
      " 26: vocoder_models/en/vctk/hifigan_v2\n",
      " 27: vocoder_models/en/sam/hifigan_v2\n",
      " 28: vocoder_models/nl/mai/parallel-wavegan\n",
      " 29: vocoder_models/de/thorsten/wavegrad\n",
      " 30: vocoder_models/de/thorsten/fullband-melgan\n",
      " 31: vocoder_models/ja/kokoro/hifigan_v1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tts_models/en/ek1/tacotron2',\n",
       " 'tts_models/en/ljspeech/tacotron2-DDC',\n",
       " 'tts_models/en/ljspeech/tacotron2-DDC_ph',\n",
       " 'tts_models/en/ljspeech/glow-tts',\n",
       " 'tts_models/en/ljspeech/speedy-speech',\n",
       " 'tts_models/en/ljspeech/tacotron2-DCA',\n",
       " 'tts_models/en/ljspeech/vits',\n",
       " 'tts_models/en/ljspeech/fast_pitch',\n",
       " 'tts_models/en/vctk/sc-glow-tts',\n",
       " 'tts_models/en/vctk/vits',\n",
       " 'tts_models/en/vctk/fast_pitch',\n",
       " 'tts_models/en/sam/tacotron-DDC',\n",
       " 'tts_models/es/mai/tacotron2-DDC',\n",
       " 'tts_models/fr/mai/tacotron2-DDC',\n",
       " 'tts_models/uk/mai/glow-tts',\n",
       " 'tts_models/zh-CN/baker/tacotron2-DDC-GST',\n",
       " 'tts_models/nl/mai/tacotron2-DDC',\n",
       " 'tts_models/de/thorsten/tacotron2-DCA',\n",
       " 'tts_models/ja/kokoro/tacotron2-DDC',\n",
       " 'vocoder_models/universal/libri-tts/wavegrad',\n",
       " 'vocoder_models/universal/libri-tts/fullband-melgan',\n",
       " 'vocoder_models/en/ek1/wavegrad',\n",
       " 'vocoder_models/en/ljspeech/multiband-melgan',\n",
       " 'vocoder_models/en/ljspeech/hifigan_v2',\n",
       " 'vocoder_models/en/ljspeech/univnet',\n",
       " 'vocoder_models/en/vctk/hifigan_v2',\n",
       " 'vocoder_models/en/sam/hifigan_v2',\n",
       " 'vocoder_models/nl/mai/parallel-wavegan',\n",
       " 'vocoder_models/de/thorsten/wavegrad',\n",
       " 'vocoder_models/de/thorsten/fullband-melgan',\n",
       " 'vocoder_models/ja/kokoro/hifigan_v1']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:08:14.636816Z",
     "start_time": "2024-09-25T07:08:14.588259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TTS.tts.configs.align_tts_config import AlignTTSConfig\n",
    "config = AlignTTSConfig()\n",
    "config"
   ],
   "id": "79e4bbc67b8b7b07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlignTTSConfig(model='align_tts', run_name='coqui_tts', run_description='', epochs=10000, batch_size=None, eval_batch_size=None, mixed_precision=False, scheduler_after_epoch=False, run_eval=True, test_delay_epochs=0, print_eval=False, dashboard_logger='tensorboard', print_step=25, plot_step=100, model_param_stats=False, project_name=None, log_model_step=None, wandb_entity=None, save_step=10000, checkpoint=True, keep_all_best=False, keep_after=10000, num_loader_workers=0, num_eval_loader_workers=0, use_noise_augment=False, output_path=None, distributed_backend='nccl', distributed_url='tcp://localhost:54321', audio=BaseAudioConfig(fft_size=1024, win_length=1024, hop_length=256, frame_shift_ms=None, frame_length_ms=None, stft_pad_mode='reflect', sample_rate=22050, resample=False, preemphasis=0.0, ref_level_db=20, do_sound_norm=False, log_func='np.log10', do_trim_silence=True, trim_db=45, power=1.5, griffin_lim_iters=60, num_mels=80, mel_fmin=0.0, mel_fmax=None, spec_gain=20, do_amp_to_db_linear=True, do_amp_to_db_mel=True, signal_norm=True, min_level_db=-100, symmetric_norm=True, max_norm=4.0, clip_norm=True, stats_path=None), use_phonemes=False, use_espeak_phonemes=True, phoneme_language=None, compute_input_seq_cache=False, text_cleaner=None, enable_eos_bos_chars=False, test_sentences_file='', phoneme_cache_path=None, characters=None, batch_group_size=0, loss_masking=None, sort_by_audio_len=False, min_seq_len=13, max_seq_len=200, compute_f0=False, compute_linear_spec=False, add_blank=False, datasets=[BaseDatasetConfig(name='', path='', meta_file_train='', ununsed_speakers=None, meta_file_val='', meta_file_attn_mask='')], optimizer='Adam', optimizer_params={'betas': [0.9, 0.998], 'weight_decay': 1e-06}, lr_scheduler=None, lr_scheduler_params=None, test_sentences=[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", 'Be a voice, not an echo.', \"I'm sorry Dave. I'm afraid I can't do that.\", \"This cake is great. It's so delicious and moist.\", 'Prior to November 22, 1963.'], model_args=AlignTTSArgs(num_chars=None, out_channels=80, hidden_channels=256, hidden_channels_dp=256, encoder_type='fftransformer', encoder_params={'hidden_channels_ffn': 1024, 'num_heads': 2, 'num_layers': 6, 'dropout_p': 0.1}, decoder_type='fftransformer', decoder_params={'hidden_channels_ffn': 1024, 'num_heads': 2, 'num_layers': 6, 'dropout_p': 0.1}, length_scale=1.0, num_speakers=0, use_speaker_embedding=False, use_d_vector_file=False, d_vector_dim=0), phase_start_steps=None, ssim_alpha=1.0, spec_loss_alpha=1.0, dur_loss_alpha=1.0, mdn_alpha=1.0, use_speaker_embedding=False, use_d_vector_file=False, d_vector_file=False, lr=0.0001, grad_clip=5.0, r=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:06:08.241349Z",
     "start_time": "2024-09-25T07:06:08.224298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "config = Tacotron2Config()\n",
    "config"
   ],
   "id": "fe17fb5107d33a87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tacotron2Config(model='tacotron2', run_name='coqui_tts', run_description='', epochs=10000, batch_size=None, eval_batch_size=None, mixed_precision=False, scheduler_after_epoch=False, run_eval=True, test_delay_epochs=0, print_eval=False, dashboard_logger='tensorboard', print_step=25, plot_step=100, model_param_stats=False, project_name=None, log_model_step=None, wandb_entity=None, save_step=10000, checkpoint=True, keep_all_best=False, keep_after=10000, num_loader_workers=0, num_eval_loader_workers=0, use_noise_augment=False, output_path=None, distributed_backend='nccl', distributed_url='tcp://localhost:54321', audio=BaseAudioConfig(fft_size=1024, win_length=1024, hop_length=256, frame_shift_ms=None, frame_length_ms=None, stft_pad_mode='reflect', sample_rate=22050, resample=False, preemphasis=0.0, ref_level_db=20, do_sound_norm=False, log_func='np.log10', do_trim_silence=True, trim_db=45, power=1.5, griffin_lim_iters=60, num_mels=80, mel_fmin=0.0, mel_fmax=None, spec_gain=20, do_amp_to_db_linear=True, do_amp_to_db_mel=True, signal_norm=True, min_level_db=-100, symmetric_norm=True, max_norm=4.0, clip_norm=True, stats_path=None), use_phonemes=False, use_espeak_phonemes=True, phoneme_language=None, compute_input_seq_cache=False, text_cleaner=None, enable_eos_bos_chars=False, test_sentences_file='', phoneme_cache_path=None, characters=None, batch_group_size=0, loss_masking=True, sort_by_audio_len=False, min_seq_len=1, max_seq_len=inf, compute_f0=False, compute_linear_spec=False, add_blank=False, datasets=[BaseDatasetConfig(name='', path='', meta_file_train='', ununsed_speakers=None, meta_file_val='', meta_file_attn_mask='')], optimizer='RAdam', optimizer_params={'betas': [0.9, 0.998], 'weight_decay': 1e-06}, lr_scheduler='NoamLR', lr_scheduler_params={'warmup_steps': 4000}, test_sentences=[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", 'Be a voice, not an echo.', \"I'm sorry Dave. I'm afraid I can't do that.\", \"This cake is great. It's so delicious and moist.\", 'Prior to November 22, 1963.'], use_gst=False, gst=None, gst_style_input=None, num_speakers=1, num_chars=0, r=2, gradual_training=None, memory_size=-1, prenet_type='original', prenet_dropout=True, prenet_dropout_at_inference=False, stopnet=True, separate_stopnet=True, stopnet_pos_weight=10.0, max_decoder_steps=500, encoder_in_features=512, decoder_in_features=512, decoder_output_dim=80, out_channels=80, attention_type='original', attention_heads=None, attention_norm='sigmoid', attention_win=False, windowing=False, use_forward_attn=False, forward_attn_mask=False, transition_agent=False, location_attn=True, bidirectional_decoder=False, double_decoder_consistency=False, ddc_r=6, use_speaker_embedding=False, speaker_embedding_dim=512, use_d_vector_file=False, d_vector_file=False, d_vector_dim=None, lr=0.0001, grad_clip=5.0, seq_len_norm=False, decoder_loss_alpha=0.25, postnet_loss_alpha=0.25, postnet_diff_spec_alpha=0.25, decoder_diff_spec_alpha=0.25, decoder_ssim_alpha=0.25, postnet_ssim_alpha=0.25, ga_alpha=5.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:03:32.714071Z",
     "start_time": "2024-09-25T07:03:32.697594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TTS.tts.configs.tacotron_config import TacotronConfig\n",
    "config = TacotronConfig()\n",
    "config"
   ],
   "id": "48330b69ffdf8220",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TacotronConfig(model='tacotron', run_name='coqui_tts', run_description='', epochs=10000, batch_size=None, eval_batch_size=None, mixed_precision=False, scheduler_after_epoch=False, run_eval=True, test_delay_epochs=0, print_eval=False, dashboard_logger='tensorboard', print_step=25, plot_step=100, model_param_stats=False, project_name=None, log_model_step=None, wandb_entity=None, save_step=10000, checkpoint=True, keep_all_best=False, keep_after=10000, num_loader_workers=0, num_eval_loader_workers=0, use_noise_augment=False, output_path=None, distributed_backend='nccl', distributed_url='tcp://localhost:54321', audio=BaseAudioConfig(fft_size=1024, win_length=1024, hop_length=256, frame_shift_ms=None, frame_length_ms=None, stft_pad_mode='reflect', sample_rate=22050, resample=False, preemphasis=0.0, ref_level_db=20, do_sound_norm=False, log_func='np.log10', do_trim_silence=True, trim_db=45, power=1.5, griffin_lim_iters=60, num_mels=80, mel_fmin=0.0, mel_fmax=None, spec_gain=20, do_amp_to_db_linear=True, do_amp_to_db_mel=True, signal_norm=True, min_level_db=-100, symmetric_norm=True, max_norm=4.0, clip_norm=True, stats_path=None), use_phonemes=False, use_espeak_phonemes=True, phoneme_language=None, compute_input_seq_cache=False, text_cleaner=None, enable_eos_bos_chars=False, test_sentences_file='', phoneme_cache_path=None, characters=None, batch_group_size=0, loss_masking=True, sort_by_audio_len=False, min_seq_len=1, max_seq_len=inf, compute_f0=False, compute_linear_spec=False, add_blank=False, datasets=[BaseDatasetConfig(name='', path='', meta_file_train='', ununsed_speakers=None, meta_file_val='', meta_file_attn_mask='')], optimizer='RAdam', optimizer_params={'betas': [0.9, 0.998], 'weight_decay': 1e-06}, lr_scheduler='NoamLR', lr_scheduler_params={'warmup_steps': 4000}, test_sentences=[\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", 'Be a voice, not an echo.', \"I'm sorry Dave. I'm afraid I can't do that.\", \"This cake is great. It's so delicious and moist.\", 'Prior to November 22, 1963.'], use_gst=False, gst=None, gst_style_input=None, num_speakers=1, num_chars=0, r=2, gradual_training=None, memory_size=-1, prenet_type='original', prenet_dropout=True, prenet_dropout_at_inference=False, stopnet=True, separate_stopnet=True, stopnet_pos_weight=10.0, max_decoder_steps=500, encoder_in_features=256, decoder_in_features=256, decoder_output_dim=80, out_channels=513, attention_type='original', attention_heads=None, attention_norm='sigmoid', attention_win=False, windowing=False, use_forward_attn=False, forward_attn_mask=False, transition_agent=False, location_attn=True, bidirectional_decoder=False, double_decoder_consistency=False, ddc_r=6, use_speaker_embedding=False, speaker_embedding_dim=512, use_d_vector_file=False, d_vector_file=False, d_vector_dim=None, lr=0.0001, grad_clip=5.0, seq_len_norm=False, decoder_loss_alpha=0.25, postnet_loss_alpha=0.25, postnet_diff_spec_alpha=0.25, decoder_diff_spec_alpha=0.25, decoder_ssim_alpha=0.25, postnet_ssim_alpha=0.25, ga_alpha=5.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T02:39:08.444269Z",
     "start_time": "2024-09-27T02:08:03.162887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "A script to download the InfoRE dataset and textgrid files.\n",
    "\"\"\"\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pooch\n",
    "from pooch import Unzip\n",
    "from tqdm.cli import tqdm\n",
    "\n",
    "\n",
    "def download_infore_data():\n",
    "    \"\"\"download infore wav files\"\"\"\n",
    "    files = pooch.retrieve(\n",
    "        url=\"https://huggingface.co/datasets/ntt123/infore/resolve/main/infore_16k_denoised.zip\",\n",
    "        known_hash=\"2445527b345fb0b1816ce3c8f09bae419d6bbe251f16d6c74d8dd95ef9fb0737\",\n",
    "        processor=Unzip(),\n",
    "        progressbar=True,\n",
    "    )\n",
    "    data_dir = Path(sorted(files)[0]).parent\n",
    "    return data_dir\n",
    "\n",
    "\n",
    "def download_textgrid():\n",
    "    \"\"\"download textgrid files\"\"\"\n",
    "    files = pooch.retrieve(\n",
    "        url=\"https://huggingface.co/datasets/ntt123/infore/resolve/main/infore_tg.zip\",\n",
    "        known_hash=\"26e4f53025220097ea95dc266657de8d65104b0a17a6ffba778fc016c8dd36d7\",\n",
    "        processor=Unzip(),\n",
    "        progressbar=True,\n",
    "    )\n",
    "    data_dir = Path(sorted(files)[0]).parent\n",
    "    return data_dir\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"./train_data\")\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "wav_dir = download_infore_data()\n",
    "tg_dir = download_textgrid()\n",
    "\n",
    "for path in tqdm(tg_dir.glob(\"*.TextGrid\")):\n",
    "    wav_name = path.with_suffix(\".wav\").name\n",
    "    wav_src = wav_dir / wav_name\n",
    "    shutil.copy(path, DATA_ROOT)\n",
    "    shutil.copy(wav_src, DATA_ROOT)\n"
   ],
   "id": "97be72d741ad4c9c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data from 'https://huggingface.co/datasets/ntt123/infore/resolve/main/infore_16k_denoised.zip' to file 'C:\\Users\\ADMIN\\AppData\\Local\\pooch\\pooch\\Cache\\f3bae51a3fb4b136a820022b97ff179a-infore_16k_denoised.zip'.\n",
      "100%|#####################################| 2.29G/2.29G [00:00<00:00, 1.15TB/s]\n",
      "Unzipping contents of 'C:\\Users\\ADMIN\\AppData\\Local\\pooch\\pooch\\Cache\\f3bae51a3fb4b136a820022b97ff179a-infore_16k_denoised.zip' to 'C:\\Users\\ADMIN\\AppData\\Local\\pooch\\pooch\\Cache\\f3bae51a3fb4b136a820022b97ff179a-infore_16k_denoised.zip.unzip'\n",
      "Downloading data from 'https://huggingface.co/datasets/ntt123/infore/resolve/main/infore_tg.zip' to file 'C:\\Users\\ADMIN\\AppData\\Local\\pooch\\pooch\\Cache\\637ecb70b6e3c019d67a16c77a3269bf-infore_tg.zip'.\n",
      "100%|#####################################| 21.1M/21.1M [00:00<00:00, 21.7GB/s]\n",
      "Unzipping contents of 'C:\\Users\\ADMIN\\AppData\\Local\\pooch\\pooch\\Cache\\637ecb70b6e3c019d67a16c77a3269bf-infore_tg.zip' to 'C:\\Users\\ADMIN\\AppData\\Local\\pooch\\pooch\\Cache\\637ecb70b6e3c019d67a16c77a3269bf-infore_tg.zip.unzip'\n",
      "14829it [07:34, 32.61it/s]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TEST",
   "id": "91fe55e041bb7508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T04:41:41.537433Z",
     "start_time": "2024-09-30T04:41:38.354858Z"
    }
   },
   "cell_type": "code",
   "source": "import waveglow",
   "id": "caea7e270d58e6e9",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WaveGlow' from 'waveglow' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwaveglow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WaveGlow\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'WaveGlow' from 'waveglow' (unknown location)"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T04:12:59.859057Z",
     "start_time": "2024-09-30T04:12:59.853435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os"
   ],
   "id": "50c8866b458c2f27",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T09:58:40.988656Z",
     "start_time": "2024-10-02T02:45:22.185758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tgt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tokenize(text):\n",
    "    return [vocab.get(char, vocab[\"<unk>\"]) for char in text]  # char -> index\n",
    "\n",
    "# build vocabulary\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}  # add <pad> and <unk>\n",
    "for i, char in enumerate(\"abcdefghijklmnopqrstuvwxyz \", 2):\n",
    "    vocab[char] = i\n",
    "\n",
    "class TTS_Dataset(Dataset):\n",
    "    def __init__(self, text_files, audio_files):\n",
    "        self.text_files = text_files\n",
    "        self.audio_files = audio_files\n",
    "        self.mel_transform = MelSpectrogram(sample_rate=16000, n_mels=80)\n",
    "        assert len(self.text_files) == len(self.audio_files), \"Mismatch between text and audio data lengths.\"\n",
    "\n",
    "    def process_text(self, text_file):\n",
    "        tg = tgt.io.read_textgrid(text_file, include_empty_intervals=True)\n",
    "        if tg.get_tier_names():\n",
    "            tier = tg.get_tier_by_name(tg.get_tier_names()[0])\n",
    "            text = \" \".join([interval.text for interval in tier.intervals])\n",
    "            return text\n",
    "        return \"\"\n",
    "\n",
    "    def process_audio(self, audio_file):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        target_sample_rate = 16000\n",
    "        if sample_rate != target_sample_rate:\n",
    "            waveform = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)(waveform)\n",
    "        mel_spec = self.mel_transform(waveform)\n",
    "        return mel_spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.process_text(self.text_files[idx])\n",
    "        audio = self.process_audio(self.audio_files[idx])\n",
    "        return text, audio\n",
    "\n",
    "class TTSModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, output_dim, num_lstm_layers, dropout):\n",
    "        super(TTSModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Multi-layer LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=num_lstm_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(lstm_hidden_dim, lstm_hidden_dim)\n",
    "        \n",
    "        # Fully connected layer with dropout\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        embedded = self.embedding(text_input)  # Embedding\n",
    "        lstm_out, _ = self.lstm(embedded)     # LSTM output\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        attended_lstm_out = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Fully connected with dropout\n",
    "        output = self.fc(self.dropout(attended_lstm_out))\n",
    "        return output\n",
    "\n",
    "# Ensure the target is reshaped similarly\n",
    "def collate_fn(batch):\n",
    "    text_data, audio_data = zip(*batch)\n",
    "\n",
    "    # Tokenize texts and pad them\n",
    "    tokenized_texts = [torch.tensor(tokenize(text), dtype=torch.long) for text in text_data]\n",
    "    padded_texts = pad_sequence(tokenized_texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "\n",
    "    # Pad audio (mel spectrograms)\n",
    "    max_length = max([audio.size(2) for audio in audio_data])\n",
    "    padded_audio = [torch.nn.functional.pad(audio, (0, max_length - audio.size(2))) for audio in audio_data]\n",
    "    padded_audio = torch.stack(padded_audio)\n",
    "\n",
    "    return padded_texts, padded_audio\n",
    "\n",
    "# Debugging: Print shapes inside training loop\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_values = []  # Store loss for each epoch\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Reshape targets to match the outputs\n",
    "            if outputs.dim() < targets.dim():\n",
    "                outputs = outputs.unsqueeze(-1)\n",
    "\n",
    "            # Resize outputs to match targets shape if necessary\n",
    "            if outputs.shape != targets.shape:\n",
    "                outputs = F.interpolate(outputs, size=targets.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        loss_values.append(epoch_loss)  # Append epoch loss\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "    # Plot loss after training\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_values, label=\"Training Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "folder_path = 'train_data'\n",
    "text_data = []  \n",
    "audio_data = []  \n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.TextGrid'):\n",
    "        text_data.append(os.path.join(folder_path, filename))\n",
    "    elif filename.endswith('.wav'):\n",
    "        audio_data.append(os.path.join(folder_path, filename))\n",
    "\n",
    "vocab_size=len(vocab)\n",
    "embedding_dim=512\n",
    "lstm_hidden_dim=1024\n",
    "output_dim=80\n",
    "num_lstm_layers=4\n",
    "dropout=0.3\n",
    "\n",
    "epochs=50\n",
    "dataset = TTS_Dataset(text_data, audio_data)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = TTSModel(vocab_size=vocab_size, \n",
    "                 embedding_dim=embedding_dim, \n",
    "                 lstm_hidden_dim=lstm_hidden_dim, \n",
    "                 output_dim=output_dim, \n",
    "                 num_lstm_layers=num_lstm_layers, \n",
    "                 dropout=dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, epochs)\n"
   ],
   "id": "bffe4c7b7276457d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 715])) that is different to the input size (torch.Size([16, 80, 80, 715])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 745])) that is different to the input size (torch.Size([16, 80, 80, 745])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 670])) that is different to the input size (torch.Size([16, 80, 80, 670])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 708])) that is different to the input size (torch.Size([16, 80, 80, 708])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 678])) that is different to the input size (torch.Size([16, 80, 80, 678])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 722])) that is different to the input size (torch.Size([16, 80, 80, 722])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 789])) that is different to the input size (torch.Size([16, 80, 80, 789])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 767])) that is different to the input size (torch.Size([16, 80, 80, 767])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 693])) that is different to the input size (torch.Size([16, 80, 80, 693])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 677])) that is different to the input size (torch.Size([16, 80, 80, 677])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 649])) that is different to the input size (torch.Size([16, 80, 80, 649])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 701])) that is different to the input size (torch.Size([16, 80, 80, 701])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 782])) that is different to the input size (torch.Size([16, 80, 80, 782])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 774])) that is different to the input size (torch.Size([16, 80, 80, 774])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 566])) that is different to the input size (torch.Size([16, 80, 80, 566])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 916])) that is different to the input size (torch.Size([16, 80, 80, 916])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 753])) that is different to the input size (torch.Size([16, 80, 80, 753])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 760])) that is different to the input size (torch.Size([16, 80, 80, 760])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 663])) that is different to the input size (torch.Size([16, 80, 80, 663])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 737])) that is different to the input size (torch.Size([16, 80, 80, 737])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 849])) that is different to the input size (torch.Size([16, 80, 80, 849])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 826])) that is different to the input size (torch.Size([16, 80, 80, 826])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 805])) that is different to the input size (torch.Size([16, 80, 80, 805])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 589])) that is different to the input size (torch.Size([16, 80, 80, 589])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 812])) that is different to the input size (torch.Size([16, 80, 80, 812])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 730])) that is different to the input size (torch.Size([16, 80, 80, 730])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 685])) that is different to the input size (torch.Size([16, 80, 80, 685])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 834])) that is different to the input size (torch.Size([16, 80, 80, 834])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 797])) that is different to the input size (torch.Size([16, 80, 80, 797])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 938])) that is different to the input size (torch.Size([16, 80, 80, 938])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 819])) that is different to the input size (torch.Size([16, 80, 80, 819])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 604])) that is different to the input size (torch.Size([16, 80, 80, 604])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 656])) that is different to the input size (torch.Size([16, 80, 80, 656])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 841])) that is different to the input size (torch.Size([16, 80, 80, 841])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 633])) that is different to the input size (torch.Size([16, 80, 80, 633])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 574])) that is different to the input size (torch.Size([16, 80, 80, 574])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 641])) that is different to the input size (torch.Size([16, 80, 80, 641])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 878])) that is different to the input size (torch.Size([16, 80, 80, 878])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 945])) that is different to the input size (torch.Size([16, 80, 80, 945])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 871])) that is different to the input size (torch.Size([16, 80, 80, 871])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 864])) that is different to the input size (torch.Size([16, 80, 80, 864])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 581])) that is different to the input size (torch.Size([16, 80, 80, 581])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 893])) that is different to the input size (torch.Size([16, 80, 80, 893])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 618])) that is different to the input size (torch.Size([16, 80, 80, 618])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 886])) that is different to the input size (torch.Size([16, 80, 80, 886])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 930])) that is different to the input size (torch.Size([16, 80, 80, 930])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 611])) that is different to the input size (torch.Size([16, 80, 80, 611])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 529])) that is different to the input size (torch.Size([16, 80, 80, 529])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 597])) that is different to the input size (torch.Size([16, 80, 80, 597])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 901])) that is different to the input size (torch.Size([16, 80, 80, 901])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 626])) that is different to the input size (torch.Size([16, 80, 80, 626])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 909])) that is different to the input size (torch.Size([16, 80, 80, 909])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 857])) that is different to the input size (torch.Size([16, 80, 80, 857])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 691])) that is different to the input size (torch.Size([16, 80, 80, 691])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 923])) that is different to the input size (torch.Size([16, 80, 80, 923])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 766])) that is different to the input size (torch.Size([16, 80, 80, 766])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 477])) that is different to the input size (torch.Size([16, 80, 80, 477])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 552])) that is different to the input size (torch.Size([16, 80, 80, 552])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 737])) that is different to the input size (torch.Size([13, 80, 80, 737])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 730.4315363648383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 753])) that is different to the input size (torch.Size([13, 80, 80, 753])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 724.0280683367255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 545])) that is different to the input size (torch.Size([16, 80, 80, 545])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 812])) that is different to the input size (torch.Size([13, 80, 80, 812])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 725.9529539764635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 559])) that is different to the input size (torch.Size([16, 80, 80, 559])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 760])) that is different to the input size (torch.Size([13, 80, 80, 760])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 723.714793019053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 537])) that is different to the input size (torch.Size([16, 80, 80, 537])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 671])) that is different to the input size (torch.Size([16, 80, 80, 671])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 650])) that is different to the input size (torch.Size([16, 80, 80, 650])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 834])) that is different to the input size (torch.Size([13, 80, 80, 834])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 725.4402214963845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 698])) that is different to the input size (torch.Size([16, 80, 80, 698])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 805])) that is different to the input size (torch.Size([13, 80, 80, 805])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 725.457918794667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 641])) that is different to the input size (torch.Size([13, 80, 80, 641])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 725.2033050767554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 657])) that is different to the input size (torch.Size([16, 80, 80, 657])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 819])) that is different to the input size (torch.Size([13, 80, 80, 819])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 725.0053012028995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 500])) that is different to the input size (torch.Size([16, 80, 80, 500])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 569])) that is different to the input size (torch.Size([16, 80, 80, 569])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 724.3309802535797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 701])) that is different to the input size (torch.Size([13, 80, 80, 701])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 723.5182927367242\n",
      "Epoch 11, Loss: 723.3034804260847\n",
      "Epoch 12, Loss: 723.8049307188427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 470])) that is different to the input size (torch.Size([16, 80, 80, 470])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 485])) that is different to the input size (torch.Size([16, 80, 80, 485])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 693])) that is different to the input size (torch.Size([13, 80, 80, 693])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 725.9507144076152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 522])) that is different to the input size (torch.Size([16, 80, 80, 522])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 514])) that is different to the input size (torch.Size([16, 80, 80, 514])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 722])) that is different to the input size (torch.Size([13, 80, 80, 722])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 723.2168349643613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 609])) that is different to the input size (torch.Size([16, 80, 80, 609])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 723.0628367797457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 548])) that is different to the input size (torch.Size([16, 80, 80, 548])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 649])) that is different to the input size (torch.Size([13, 80, 80, 649])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 723.1091742819052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 841])) that is different to the input size (torch.Size([13, 80, 80, 841])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 725.2642413718544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 708])) that is different to the input size (torch.Size([13, 80, 80, 708])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 724.2467423555278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 715])) that is different to the input size (torch.Size([13, 80, 80, 715])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 722.4722677187626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 493])) that is different to the input size (torch.Size([16, 80, 80, 493])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 878])) that is different to the input size (torch.Size([13, 80, 80, 878])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 723.1536936672503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 602])) that is different to the input size (torch.Size([16, 80, 80, 602])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 725.8940936667763\n",
      "Epoch 22, Loss: 722.2192181008019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16, 1, 80, 623])) that is different to the input size (torch.Size([16, 80, 80, 623])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 730])) that is different to the input size (torch.Size([13, 80, 80, 730])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 724.0211283365885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 663])) that is different to the input size (torch.Size([13, 80, 80, 663])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 724.3416102799268\n",
      "Epoch 25, Loss: 723.5998108173522\n",
      "Epoch 26, Loss: 724.4451511222568\n",
      "Epoch 27, Loss: 722.7817049654042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 552])) that is different to the input size (torch.Size([13, 80, 80, 552])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 724.0301705929326\n",
      "Epoch 29, Loss: 724.5120157614238\n",
      "Epoch 30, Loss: 724.0576336149774\n",
      "Epoch 31, Loss: 721.364540898427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([13, 1, 80, 745])) that is different to the input size (torch.Size([13, 80, 80, 745])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 723.905695778395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 150\u001B[0m\n\u001B[0;32m    147\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mMSELoss()\n\u001B[0;32m    148\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[1;32m--> 150\u001B[0m train(model, dataloader, criterion, optimizer, epochs)\n",
      "Cell \u001B[1;32mIn[3], line 91\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m     90\u001B[0m     running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 91\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m inputs, targets \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m     92\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     93\u001B[0m         targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[3], line 50\u001B[0m, in \u001B[0;36mTTS_Dataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m     49\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_text(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_files[idx])\n\u001B[1;32m---> 50\u001B[0m     audio \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_audio(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maudio_files[idx])\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text, audio\n",
      "Cell \u001B[1;32mIn[3], line 38\u001B[0m, in \u001B[0;36mTTS_Dataset.process_audio\u001B[1;34m(self, audio_file)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_audio\u001B[39m(\u001B[38;5;28mself\u001B[39m, audio_file):\n\u001B[1;32m---> 38\u001B[0m     waveform, sample_rate \u001B[38;5;241m=\u001B[39m torchaudio\u001B[38;5;241m.\u001B[39mload(audio_file)\n\u001B[0;32m     39\u001B[0m     target_sample_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16000\u001B[39m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sample_rate \u001B[38;5;241m!=\u001B[39m target_sample_rate:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:205\u001B[0m, in \u001B[0;36mget_load_func.<locals>.load\u001B[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load audio data from source.\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    204\u001B[0m backend \u001B[38;5;241m=\u001B[39m dispatcher(uri, \u001B[38;5;28mformat\u001B[39m, backend)\n\u001B[1;32m--> 205\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m backend\u001B[38;5;241m.\u001B[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001B[38;5;28mformat\u001B[39m, buffer_size)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py:27\u001B[0m, in \u001B[0;36mSoundfileBackend.load\u001B[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[0;32m     19\u001B[0m     uri: Union[BinaryIO, \u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     buffer_size: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4096\u001B[39m,\n\u001B[0;32m     26\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m---> 27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m soundfile_backend\u001B[38;5;241m.\u001B[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001B[38;5;28mformat\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:230\u001B[0m, in \u001B[0;36mload\u001B[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001B[0m\n\u001B[0;32m    227\u001B[0m         dtype \u001B[38;5;241m=\u001B[39m _SUBTYPE2DTYPE[file_\u001B[38;5;241m.\u001B[39msubtype]\n\u001B[0;32m    229\u001B[0m     frames \u001B[38;5;241m=\u001B[39m file_\u001B[38;5;241m.\u001B[39m_prepare_read(frame_offset, \u001B[38;5;28;01mNone\u001B[39;00m, num_frames)\n\u001B[1;32m--> 230\u001B[0m     waveform \u001B[38;5;241m=\u001B[39m file_\u001B[38;5;241m.\u001B[39mread(frames, dtype, always_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    231\u001B[0m     sample_rate \u001B[38;5;241m=\u001B[39m file_\u001B[38;5;241m.\u001B[39msamplerate\n\u001B[0;32m    233\u001B[0m waveform \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(waveform)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\soundfile.py:895\u001B[0m, in \u001B[0;36mSoundFile.read\u001B[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001B[0m\n\u001B[0;32m    893\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frames \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m frames \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(out):\n\u001B[0;32m    894\u001B[0m         frames \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(out)\n\u001B[1;32m--> 895\u001B[0m frames \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_array_io(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m'\u001B[39m, out, frames)\n\u001B[0;32m    896\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m>\u001B[39m frames:\n\u001B[0;32m    897\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fill_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\soundfile.py:1344\u001B[0m, in \u001B[0;36mSoundFile._array_io\u001B[1;34m(self, action, array, frames)\u001B[0m\n\u001B[0;32m   1342\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m array\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mitemsize \u001B[38;5;241m==\u001B[39m _ffi\u001B[38;5;241m.\u001B[39msizeof(ctype)\n\u001B[0;32m   1343\u001B[0m cdata \u001B[38;5;241m=\u001B[39m _ffi\u001B[38;5;241m.\u001B[39mcast(ctype \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m, array\u001B[38;5;241m.\u001B[39m__array_interface__[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m-> 1344\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cdata_io(action, cdata, ctype, frames)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\soundfile.py:1353\u001B[0m, in \u001B[0;36mSoundFile._cdata_io\u001B[1;34m(self, action, data, ctype, frames)\u001B[0m\n\u001B[0;32m   1351\u001B[0m     curr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtell()\n\u001B[0;32m   1352\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(_snd, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msf_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m action \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m ctype)\n\u001B[1;32m-> 1353\u001B[0m frames \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file, data, frames)\n\u001B[0;32m   1354\u001B[0m _error_check(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_errorcode)\n\u001B[0;32m   1355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseekable():\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T11:54:41.405367Z",
     "start_time": "2024-10-01T11:54:41.373774Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'dcom_tts.pth')",
   "id": "cec157fbb73e05f1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:42:33.972861Z",
     "start_time": "2024-10-04T02:42:33.958512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# back up mseloss\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_values = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Expecting (batch_size, seq_len, output_dim)\n",
    "            \n",
    "            # Check dimensions\n",
    "            if outputs.dim() != 3:\n",
    "                raise ValueError(f\"Expected outputs to have 3 dimensions, got {outputs.dim()}\")\n",
    "\n",
    "            # If targets have 4 dimensions, squeeze the first dimension\n",
    "            if targets.dim() == 4:\n",
    "                targets = targets.squeeze(1)  # Make sure targets are (batch_size, seq_len, output_dim)\n",
    "\n",
    "            # Ensure sequence lengths match\n",
    "            if outputs.size(1) != targets.size(1):\n",
    "                targets = F.interpolate(targets.permute(0, 2, 1), size=outputs.size(1), mode='linear', align_corners=True)\n",
    "                targets = targets.permute(0, 2, 1)\n",
    "\n",
    "            # Ensure output dimensions match\n",
    "            if outputs.size(2) != targets.size(2):\n",
    "                targets = F.interpolate(targets.permute(0, 1, 2), size=outputs.size(2), mode='linear', align_corners=True)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        loss_values.append(epoch_loss)\n",
    "        scheduler.step() \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "    # Plot loss after training\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_values, label=\"Training Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# back up ctcloss   \n",
    "def train(model, dataloader, criterion, optimizer, scheduler, num_epochs, patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_values = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "            log_probs = F.log_softmax(outputs, dim=2)\n",
    "\n",
    "            input_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.long).to(device)\n",
    "            target_lengths = torch.tensor([len(t[t != vocab[\"<pad>\"]]) for t in targets], dtype=torch.long).to(device)\n",
    "            targets = targets[targets != vocab[\"<pad>\"]].view(-1).to(device)\n",
    "\n",
    "            try:\n",
    "                loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                continue\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        loss_values.append(epoch_loss)\n",
    "        \n",
    "        # Pass the epoch_loss to the scheduler\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping due to no improvement\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(loss_values) + 1), loss_values, label=\"Training Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "bbe62ee6f19a98c5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T05:18:08.762947Z",
     "start_time": "2024-10-11T03:35:20.692132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torchaudio.transforms import PitchShift\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenizer & Vocabulary\n",
    "def tokenize(text):\n",
    "    return [vocab.get(char, vocab[\"<unk>\"]) for char in text.lower()]\n",
    "\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for i, char in enumerate(\"abcdefghijklmnopqrstuvwxyz \", 2):\n",
    "    vocab[char] = i\n",
    "\n",
    "# Dataset class\n",
    "class CommonVoiceDataset(Dataset):\n",
    "    def __init__(self, tsv_files, audio_folder):\n",
    "        # Combine multiple TSV files into a single dataframe\n",
    "        self.data = pd.concat([pd.read_csv(tsv_file, sep='\\t') for tsv_file in tsv_files])\n",
    "        self.audio_folder = audio_folder\n",
    "        self.mel_transform = MelSpectrogram(sample_rate=16000, n_mels=80)\n",
    "    \n",
    "    def process_audio(self, audio_file):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        if waveform.abs().sum() == 0:\n",
    "            print(f\"Silent or all-zero audio: {audio_file}\")\n",
    "        \n",
    "        target_sample_rate = 16000\n",
    "        if sample_rate != target_sample_rate:\n",
    "            waveform = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)(waveform)\n",
    "        \n",
    "        # Data augmentation: add noise and pitch shift\n",
    "        noise = torch.randn_like(waveform) * 0.005\n",
    "        augmented_waveform = waveform + noise\n",
    "        \n",
    "        # Time out this for now (recommended ram usage is 32gb)\n",
    "        # try:\n",
    "        #     augmented_waveform = torchaudio.transforms.PitchShift(sample_rate=target_sample_rate, n_steps=2)(augmented_waveform)\n",
    "        # # except Exception as e:\n",
    "        # #     print(f\"PitchShift error: {e}\")\n",
    "        # except RuntimeError as e:\n",
    "        #     print(f\"PitchShift error: {e} Skipping this augmentation.\")\n",
    "        \n",
    "        mel_spec = self.mel_transform(augmented_waveform)\n",
    "        epsilon = 1e-10\n",
    "        mel_spec = torch.log(mel_spec + epsilon)\n",
    "        \n",
    "        if torch.isnan(mel_spec).any() or torch.isinf(mel_spec).any():\n",
    "            print(f\"NaN or Inf in mel_spec for {audio_file}\")\n",
    "            mel_spec = torch.nan_to_num(mel_spec, nan=0.0, posinf=1e3, neginf=-1e3)\n",
    "        \n",
    "        return mel_spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['sentence']\n",
    "        audio_path = os.path.join(self.audio_folder, self.data.iloc[idx]['path'])\n",
    "        audio = self.process_audio(audio_path)\n",
    "        return text, audio\n",
    "\n",
    "# Model class\n",
    "class TTSModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_hidden_dim, output_dim, num_lstm_layers, dropout): #can modify it\n",
    "        super(TTSModel, self).__init__() \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # Embedding layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, num_layers=num_lstm_layers, batch_first=True, dropout=dropout) # LSTM layer\n",
    "        self.lstm_layer = nn.Linear(lstm_hidden_dim, lstm_hidden_dim) # Apply linear transformation to LSTM output\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=lstm_hidden_dim, num_heads=8, dropout=dropout) # Attention mechanism\n",
    "        self.attention_layer = nn.Linear(lstm_hidden_dim, lstm_hidden_dim) # Apply linear transformation to attention output\n",
    "        self.norm = nn.LayerNorm(lstm_hidden_dim)  # Normalization layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim) # Fully connected output layer\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout\n",
    "\n",
    "    def forward(self, text_input): # can modify it\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(text_input)\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # Apply linear transformation to LSTM output\n",
    "        lstm_out = self.lstm_layer(lstm_out)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "\n",
    "        # Residual connection and normalization\n",
    "        lstm_out = lstm_out + attn_output  # Residual connection\n",
    "        lstm_out = self.norm(lstm_out)     # Apply normalization\n",
    "\n",
    "        # Fully connected output layer\n",
    "        output = self.fc(self.dropout(lstm_out))\n",
    "        return output\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    text_data, audio_data = zip(*batch)\n",
    "    \n",
    "    # Tokenize and pad texts (inputs)\n",
    "    tokenized_texts = [torch.tensor(tokenize(text), dtype=torch.long) for text in text_data]\n",
    "    padded_texts = pad_sequence(tokenized_texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    \n",
    "    # Pad audio spectrograms (targets)\n",
    "    max_audio_len = max([audio.size(-1) for audio in audio_data])  # Max length of the time dimension\n",
    "    padded_audio = [F.pad(audio, (0, max_audio_len - audio.size(-1))) for audio in audio_data]\n",
    "    padded_audio = torch.stack(padded_audio)\n",
    "    \n",
    "    # Create masks for the padded regions\n",
    "    text_lengths = [len(t) for t in tokenized_texts]\n",
    "    audio_lengths = [audio.size(-1) for audio in audio_data]\n",
    "    \n",
    "    text_mask = torch.ones(padded_texts.shape, dtype=torch.bool)\n",
    "    for i, length in enumerate(text_lengths):\n",
    "        text_mask[i, length:] = False\n",
    "    \n",
    "    audio_mask = torch.ones(padded_audio.shape[:-1], dtype=torch.bool)\n",
    "    for i, length in enumerate(audio_lengths):\n",
    "        audio_mask[i, length:] = False\n",
    "    \n",
    "    return padded_texts, padded_audio, text_mask, audio_mask\n",
    "\n",
    "# Training function with early stopping\n",
    "def masked_loss(outputs, targets, mask, criterion):\n",
    "    # modify targets to have shape (batch_size, seq_len, output_dim)\n",
    "    targets = targets.squeeze(1)  # detach the first dimension (batch_size)\n",
    "    targets = targets[:, :outputs.size(1), :]  # fix the second dimension (seq_len)\n",
    "    targets = targets[:, :, :outputs.size(2)]\n",
    "\n",
    "    # extract mask from targets\n",
    "    mask = mask.squeeze(1).squeeze(-1)\n",
    "    mask = mask[:, :outputs.size(1)]\n",
    "    mask = mask.unsqueeze(-1).expand_as(outputs)  \n",
    "\n",
    "    # use it\n",
    "    masked_outputs = outputs * mask  \n",
    "    masked_targets = targets * mask  \n",
    "\n",
    "    # loss\n",
    "    loss = criterion(masked_outputs, masked_targets)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, num_epochs, patience): # add patience just in case\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_values = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_mae = 0.0\n",
    "        \n",
    "        for inputs, targets, input_mask, target_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            input_mask = input_mask.to(device) # just to be sure\n",
    "            target_mask = target_mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # modify targets to have shape (batch_size, seq_len, output_dim)\n",
    "            targets = targets.squeeze(1)\n",
    "            targets = targets[:, :outputs.size(1), :]\n",
    "            targets = targets[:, :, :outputs.size(2)]\n",
    "            \n",
    "            loss = masked_loss(outputs, targets, target_mask, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            mae = torch.abs(outputs - targets).mean().item()\n",
    "            running_mae += mae\n",
    "            \n",
    "            print(f\"Epoch: {epoch+1}|{num_epochs}, Input: {inputs.size()}, Target: {targets.size()}\") # use this for checking the shapes\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader) # MSE\n",
    "        epoch_mae = running_mae / len(dataloader) # MAE\n",
    "        \n",
    "        loss_values.append(epoch_loss)\n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, MSELoss: {epoch_loss}, MAELoss: {epoch_mae}\")\n",
    "\n",
    "    # Plot loss after training\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_values, label=\"Training Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d): \n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "                # Set forget gate bias to 1 for better training\n",
    "                param.data[m.hidden_size:m.hidden_size*2].fill_(1)\n",
    "                \n",
    "# Initialize model and optimizer\n",
    "tsv_files = [\n",
    "    'cv-corpus-19.0-2024-09-13/vi/train.tsv',\n",
    "    'cv-corpus-19.0-2024-09-13/vi/validated.tsv'\n",
    "    ]\n",
    "audio_folder = 'cv-corpus-19.0-2024-09-13/vi/clips/'\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 1024\n",
    "lstm_hidden_dim = 2048\n",
    "output_dim = 80\n",
    "num_lstm_layers = 8\n",
    "dropout = 0.7\n",
    "patience=5\n",
    "\n",
    "# Epochs\n",
    "# epochs = 50 # Training epochs # ready to aboat\n",
    "# epochs = 10 # for short training\n",
    "epochs = 5 # for tasting # delicous\n",
    "# epochs = 1 # looking for something good\n",
    "\n",
    "dataset = CommonVoiceDataset(tsv_files, audio_folder)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn) # 64 batch size is enough or 128 or 256 if ur ram is big\n",
    "\n",
    "model = TTSModel(vocab_size=vocab_size, \n",
    "                 embedding_dim=embedding_dim, \n",
    "                 lstm_hidden_dim=lstm_hidden_dim,\n",
    "                 output_dim=output_dim, \n",
    "                 num_lstm_layers=num_lstm_layers, \n",
    "                 dropout=dropout)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "# criterion = nn.CTCLoss(blank=0)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4, amsgrad=True, betas=(0.9, 0.999)) # low learning rate maybe it will find something better\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=0.1) # maybe use it or not\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1) # recommended\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, scheduler, epochs, patience) # recommended: ram 32gb, gpu > 6 gb vram (cuda), may be i5 or i7, dont use laptop to train\n"
   ],
   "id": "da02c8204c2fb42a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1|5, Input: torch.Size([64, 60]), Target: torch.Size([64, 60, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 57]), Target: torch.Size([64, 57, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 66]), Target: torch.Size([64, 66, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 60]), Target: torch.Size([64, 60, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 68]), Target: torch.Size([64, 68, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 68]), Target: torch.Size([64, 68, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 54]), Target: torch.Size([64, 54, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 62]), Target: torch.Size([64, 62, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 68]), Target: torch.Size([64, 68, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 65]), Target: torch.Size([64, 65, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 56]), Target: torch.Size([64, 56, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 58]), Target: torch.Size([64, 58, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 66]), Target: torch.Size([64, 66, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 57]), Target: torch.Size([64, 57, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 56]), Target: torch.Size([64, 56, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 73]), Target: torch.Size([64, 73, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 68]), Target: torch.Size([64, 68, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 56]), Target: torch.Size([64, 56, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 65]), Target: torch.Size([64, 65, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 67]), Target: torch.Size([64, 67, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 70]), Target: torch.Size([64, 70, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 57]), Target: torch.Size([64, 57, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 67]), Target: torch.Size([64, 67, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 70]), Target: torch.Size([64, 70, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 58]), Target: torch.Size([64, 58, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 56]), Target: torch.Size([64, 56, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 69]), Target: torch.Size([64, 69, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 53]), Target: torch.Size([64, 53, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 55]), Target: torch.Size([64, 55, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 57]), Target: torch.Size([64, 57, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 48]), Target: torch.Size([64, 48, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 62]), Target: torch.Size([64, 62, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 67]), Target: torch.Size([64, 67, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 69]), Target: torch.Size([64, 69, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 67]), Target: torch.Size([64, 67, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 57]), Target: torch.Size([64, 57, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 62]), Target: torch.Size([64, 62, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 66]), Target: torch.Size([64, 66, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 62]), Target: torch.Size([64, 62, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 60]), Target: torch.Size([64, 60, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 56]), Target: torch.Size([64, 56, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 62]), Target: torch.Size([64, 62, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 59]), Target: torch.Size([64, 59, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 60]), Target: torch.Size([64, 60, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 68]), Target: torch.Size([64, 68, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 68]), Target: torch.Size([64, 68, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 67]), Target: torch.Size([64, 67, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 67]), Target: torch.Size([64, 67, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 65]), Target: torch.Size([64, 65, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 65]), Target: torch.Size([64, 65, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 63]), Target: torch.Size([64, 63, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 64]), Target: torch.Size([64, 64, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 61]), Target: torch.Size([64, 61, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 56]), Target: torch.Size([64, 56, 80])\n",
      "Epoch: 1|5, Input: torch.Size([64, 55]), Target: torch.Size([64, 55, 80])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.73 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 253\u001B[0m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\u001B[39;00m\n\u001B[0;32m    251\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mReduceLROnPlateau(optimizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m, factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 253\u001B[0m train(model, dataloader, criterion, optimizer, scheduler, epochs, patience)\n",
      "Cell \u001B[1;32mIn[1], line 171\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, criterion, optimizer, scheduler, num_epochs, patience)\u001B[0m\n\u001B[0;32m    168\u001B[0m target_mask \u001B[38;5;241m=\u001B[39m target_mask\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    170\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 171\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m    173\u001B[0m targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    174\u001B[0m targets \u001B[38;5;241m=\u001B[39m targets[:, :outputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m), :]\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[1], line 86\u001B[0m, in \u001B[0;36mTTSModel.forward\u001B[1;34m(self, text_input)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_input):\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;66;03m# Embedding layer\u001B[39;00m\n\u001B[1;32m---> 86\u001B[0m     embedded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(text_input)\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;66;03m# LSTM layer\u001B[39;00m\n\u001B[0;32m     89\u001B[0m     lstm_out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm(embedded)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39membedding(\n\u001B[0;32m    191\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[0;32m    193\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_idx,\n\u001B[0;32m    194\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_norm,\n\u001B[0;32m    195\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm_type,\n\u001B[0;32m    196\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_grad_by_freq,\n\u001B[0;32m    197\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparse,\n\u001B[0;32m    198\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2545\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2546\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2547\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2548\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2549\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2550\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2551\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39membedding(weight, \u001B[38;5;28minput\u001B[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 16.73 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T16:12:02.751588Z",
     "start_time": "2024-10-02T16:12:02.107106Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'dcom_voice_tts.pth')",
   "id": "e61cc2d56739f6a3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdcom_voice_tts.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T01:47:30.644553Z",
     "start_time": "2024-10-09T01:47:22.269316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ],
   "id": "5c5283fc1b09e682",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:33:11.497781Z",
     "start_time": "2024-10-04T02:33:11.490337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "\n",
    "# Run some things here\n",
    "\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()  # Wait for the events to be recorded!\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print('Elapsed time: {} ms'.format(elapsed_time_ms))"
   ],
   "id": "d5f0ed766c473d0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.0033599999733269215 ms\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:33:47.138172Z",
     "start_time": "2024-10-04T02:33:47.112165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cuda = torch.device('cuda')\n",
    "s = torch.cuda.Stream()  # Create a new stream.\n",
    "A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\n",
    "with torch.cuda.stream(s):\n",
    "    # sum() may start execution before normal_() finishes!\n",
    "    B = torch.sum(A)\n",
    "    torch.cuda.current_stream().wait_stream(s)\n",
    "    C = A + 1\n",
    "    D = C + 1\n",
    "    E = D + 1\n",
    "    F = E + 1\n",
    "    G = F + 1\n",
    "    H = G + 1\n",
    "    I = H + 1\n",
    "    J = I + 1\n",
    "    K = J + 1\n",
    "    L = K + 1\n",
    "    M = L + 1\n",
    "    N = M + 1\n",
    "    O = N + 1\n",
    "    P = O + 1\n",
    "    Q = P + 1\n",
    "    R = Q + 1\n",
    "    S = R + 1\n",
    "    T = S + 1\n",
    "    U = T + 1\n",
    "    V = U + 1\n",
    "    W = V + 1\n",
    "    X = W + 1\n",
    "    Y = X + 1\n",
    "    Z = Y + 1\n",
    "    print(B)"
   ],
   "id": "6d525be1b65ad8a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-56.9458, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T02:35:24.649614Z",
     "start_time": "2024-10-04T02:35:24.629426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--disable-cuda', action='store_true',\n",
    "                    help='Disable CUDA')\n",
    "args = parser.parse_args()\n",
    "args.device = None\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ],
   "id": "f0fb3e39d55929d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--disable-cuda]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\ADMIN\\AppData\\Roaming\\jupyter\\runtime\\kernel-ab2ed140-eaa3-4f82-ab9e-2ecf11fa1d51.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64eb7d3f92eef020"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
